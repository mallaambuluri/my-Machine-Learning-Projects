# -*- coding: utf-8 -*-
"""knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-9Lb0zRGHHpLo-3gmlm9sAChXK6ilHlS
"""

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import pickle



import pandas as pd
import numpy as np
import os
import os

path ='/content/drive/MyDrive/data sets/database.sqlite'

import sqlite3

con = sqlite3.connect(path)

filtered_data = pd.read_sql_query(""" SELECT * FROM Reviews WHERE Score != 3 """ , con
                                 )

def partition(x):
    if x < 3:
        return '0'
    return '1'

actualScore = filtered_data['Score']
positiveNegative =actualScore.map(partition)
filtered_data['Score'] = positiveNegative

filtered_data['Score']

display = pd.read_sql_query("""
SELECT *
FROM Reviews
WHERE Score != 3 AND UserId="AR5J8UI46CURR"
ORDER BY ProductID
""", con)

display

sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')

sorted_data

sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')
sorted_data

final=sorted_data.drop_duplicates(subset={"UserId","ProfileName","Time","Text"}, keep='first', inplace=False)

final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]

final.shape

final['Score'].value_counts()

import re

import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
import nltk
nltk.download('stopwords')

stop = set(stopwords.words('english'))
sno = nltk.stem.SnowballStemmer('english')

def cleanhtml(sentence):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, ' ', sentence)
    return cleantext
def cleanpunc(sentence):
    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sentence)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    return  cleaned
print(stop)
print('************************************')
print(sno.stem('tasty'))

final['Text'].values

i=0
str1=' '
final_string=[]
all_positive_words=[]
all_negative_words=[]
s=''
for sent in final['Text'].values:
    filtered_sentence=[]

    sent=cleanhtml(sent)
    for w in sent.split():
        for cleaned_words in cleanpunc(w).split():
            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):
                if(cleaned_words.lower() not in stop):
                    s=(sno.stem(cleaned_words.lower())).encode('utf8')
                    filtered_sentence.append(s)
                    if (final['Score'].values)[i] == 'positive':
                        all_positive_words.append(s)
                    if(final['Score'].values)[i] == 'negative':
                        all_negative_words.append(s)
                else:
                    continue
            else:
                continue

    str1 = b" ".join(filtered_sentence)
    #print("***********************************************************************")

    final_string.append(str1)
    i+=1

final['CleanedText']=final_string

final.head(5)

conn = sqlite3.connect('final.sqlite')
c=conn.cursor()
conn.text_factory = str
final.to_sql('Reviews', conn,  schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)



final = pd.read_sql_query("SELECT * FROM Reviews", conn)

from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer() #in scikit-learn
final_counts = count_vect.fit_transform(final['Text'].values)

len(final['CleanedText'])

from gensim.utils import simple_preprocess
import gensim

model_w2v = gensim.models.Word2Vec(
            final['CleanedText'],
          vector_size=200, # desired no. of features/independent variables
            window=5, # context window size
            min_count=2, # Ignores all words with total frequency lower than 2.
            hs = 0,
            negative = 10, # for negative sampling
            workers= 4, # no.of cores
            seed = 34
)

model_w2v.train(final['CleanedText'], total_examples= len(final['CleanedText']), epochs=20)

def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0
    for word in tokens:
        try:
            vec += model_w2v.wv[word].reshape((1, size))
            count += 1.
        except KeyError:  # handling the case where the token is not in vocabulary
            continue
    if count != 0:
        vec /= count
    return vec

wordvec_arrays = np.zeros((len(final['CleanedText']), 200))
for i in range(len(final['CleanedText'])):
    wordvec_arrays[i,:] = word_vector(final['CleanedText'][i], 200)
wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape

wordvec_df.to_csv('wordvec.csv',index = False)

wordvec_df

import numpy as np
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn. metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from collections import Counter
from sklearn import model_selection
from sklearn.neighbors import DistanceMetric

x=wordvec_df
x = x.head(100000)
y =final['Score'].head(100000)
y.shape
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

neighbors = np.arange(1,13)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i,k in enumerate(neighbors):
    #Setup a knn classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)

    #Fit the model
    knn.fit(x_train, y_train)
    train_accuracy[i] = knn.score(x_train, y_train)
    test_accuracy[i] = knn.score(x_test, y_test)

plt.title('k-NN Varying number of neighbors')
plt.plot(neighbors, test_accuracy, label='Testing Accuracy')
plt.plot(neighbors, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show()

knn = KNeighborsClassifier(n_neighbors=11)
#Fit the model
knn.fit(x_train,y_train)
train_accuracy1 = knn.score(x_train, y_train)
test_accuracy1 = knn.score(x_test, y_test)

print(train_accuracy1)

print(test_accuracy1)

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import accuracy_score
import numpy as np

x=wordvec_df
x = x.head(100000)
y =final['Score'].head(100000)
y.shape
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

depth = np.arange(1,20)
train_accuracy =np.empty(len(depth))
test_accuracy = np.empty(len(depth))
for i,k in enumerate(depth):
    #Setup a knn classifier with k neighbors
    clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=k, random_state=0)

    #Fit the model
    clf_gini.fit(x_train, y_train)
    train_accuracy[i] = clf_gini.score(x_train, y_train)
    test_accuracy[i] = clf_gini.score(x_test, y_test)

plt.title('decision trees Varying number of depth')
plt.plot(depth, test_accuracy, label='Testing Accuracy')
plt.plot(depth, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('max depth')
plt.ylabel('Accuracy')
plt.show()

#we know that if the depth of tree increases the model can easily overfit which means testing error is more than training error which we can observe from the above graph
# so we limit the depth of tree to #5 only

from sklearn.ensemble import RandomForestClassifier
x1=wordvec_df
x1 = x1.head(100000)
y1 =final['Score'].head(100000)
y1.shape
x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.33, random_state=42)

depth = np.arange(1,20)
train_accuracy =np.empty(len(depth))
test_accuracy = np.empty(len(depth))
for i,k in enumerate(depth):
    #Setup a knn classifier with k neighbors
    clf_gini = RandomForestClassifier(criterion='gini', max_depth=k, random_state=0)

    #Fit the model
    clf_gini.fit(x_train, y_train)
    train_accuracy[i] = clf_gini.score(x_train, y_train)
    test_accuracy[i] = clf_gini.score(x_test, y_test)



plt.title('random forest Varying number of depth')
plt.plot(depth, test_accuracy, label='Testing Accuracy')
plt.plot(depth, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('max depth')
plt.ylabel('Accuracy')
plt.show()

from sklearn.ensemble import RandomForestClassifier
x1=wordvec_df
x1 = x1.head(100000)
y1 =final['Score'].head(100000)
y1.shape
x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.33, random_state=42)

n_estimators = np.arange(100,900,100)
train_accuracy =np.empty(len(n_estimators))
test_accuracy = np.empty(len(n_estimators))
for i,k in enumerate(n_estimators):

    clf_gini = RandomForestClassifier(criterion='gini',n_estimators=k, max_depth=None, random_state=0)

    #Fit the model
    clf_gini.fit(x_train, y_train)
    train_accuracy[i] = clf_gini.score(x_train, y_train)
    test_accuracy[i] = clf_gini.score(x_test, y_test)

plt.title('random forest Varying number of n_estimators')
plt.plot(n_estimators, test_accuracy, label='Testing Accuracy')
plt.plot(n_estimators, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('n_estimators')
plt.ylabel('Accuracy')
plt.show()